{
  "swagger": "2.0",
  "info": {
    "title": "completion.proto",
    "version": "version not set"
  },
  "tags": [
    {
      "name": "Completion"
    }
  ],
  "consumes": [
    "application/json"
  ],
  "produces": [
    "application/json"
  ],
  "paths": {
    "/v1/chat/completions": {
      "post": {
        "operationId": "Completion_Chat",
        "responses": {
          "200": {
            "description": "A successful response.(streaming responses)",
            "schema": {
              "type": "object",
              "properties": {
                "result": {
                  "$ref": "#/definitions/llmChatResponse"
                },
                "error": {
                  "$ref": "#/definitions/rpcStatus"
                }
              },
              "title": "Stream result of llmChatResponse"
            }
          },
          "default": {
            "description": "An unexpected error response.",
            "schema": {
              "$ref": "#/definitions/rpcStatus"
            }
          }
        },
        "parameters": [
          {
            "name": "body",
            "in": "body",
            "required": true,
            "schema": {
              "$ref": "#/definitions/llmChatRequest"
            }
          }
        ],
        "tags": [
          "Completion"
        ]
      }
    },
    "/v1/completions": {
      "post": {
        "summary": "legacy API",
        "operationId": "Completion_Complete",
        "responses": {
          "200": {
            "description": "A successful response.(streaming responses)",
            "schema": {
              "type": "object",
              "properties": {
                "result": {
                  "$ref": "#/definitions/llmCompletionResponse"
                },
                "error": {
                  "$ref": "#/definitions/rpcStatus"
                }
              },
              "title": "Stream result of llmCompletionResponse"
            }
          },
          "default": {
            "description": "An unexpected error response.",
            "schema": {
              "$ref": "#/definitions/rpcStatus"
            }
          }
        },
        "parameters": [
          {
            "name": "body",
            "in": "body",
            "required": true,
            "schema": {
              "$ref": "#/definitions/llmCompletionRequest"
            }
          }
        ],
        "tags": [
          "Completion"
        ]
      }
    }
  },
  "definitions": {
    "llmChatChoice": {
      "type": "object",
      "properties": {
        "index": {
          "type": "integer",
          "format": "int32",
          "description": "the index of the choice in the list of choices."
        },
        "message": {
          "$ref": "#/definitions/llmChatMessage",
          "description": "A chat completion message generated by the model."
        },
        "finishReason": {
          "type": "string",
          "description": "the reason of the model stoped generating tokens.\n\"stop\" - the model hit a natural stop point or a provided stop sequence.\n\"length\" - the maximum number of tokens specified in the request was reached.\n\"function_call\" - the model called a function."
        }
      }
    },
    "llmChatMessage": {
      "type": "object",
      "properties": {
        "role": {
          "type": "string",
          "description": "the role of the messages author. One of \"system\", \"user\", \"assistant\"."
        },
        "content": {
          "type": "string",
          "description": "the content of the message. null for assistant messages with function calls."
        }
      }
    },
    "llmChatRequest": {
      "type": "object",
      "properties": {
        "id": {
          "type": "string",
          "title": "unique id for the completion request"
        },
        "messages": {
          "type": "array",
          "items": {
            "type": "object",
            "$ref": "#/definitions/llmChatMessage"
          },
          "description": "A list of messages comprising the conversation so far."
        },
        "temperature": {
          "type": "number",
          "format": "float",
          "description": "temperature of the sampling, between [0, 2]. default = 1.0\nhigher value will make the ouput more random, while a lower value will make it more deterministic.\nit is recommended altering this or top_p but not both."
        },
        "topP": {
          "type": "number",
          "format": "float",
          "title": "top_p sampling cutoff, between [0, 1.0]. default = 1.0"
        },
        "n": {
          "type": "integer",
          "format": "int64",
          "title": "number of chat completion choices to generate for each input message. default = 1"
        },
        "stream": {
          "type": "boolean",
          "title": "whether to stream partial completions back as they are generated"
        },
        "maxTokens": {
          "type": "integer",
          "format": "int32",
          "title": "the maximum number of tokens to generate in the chat completion. default = infinity"
        },
        "presencePenalty": {
          "type": "number",
          "format": "float",
          "description": "values between [-2.0, 2.0]. default = 0.0\nPositive values penalize new tokens based on their existing in the text so far, increasing \nthe model's likelihood to talk about new topics."
        },
        "frequencyPenalty": {
          "type": "number",
          "format": "float",
          "description": "values between [0.0, 2.0]. default = 0.0\nPositive values penalize new tokens based on their existing frequency in the text so far,\ndecreasing the model's likelihood to repeat the same line verbatim."
        },
        "user": {
          "type": "string",
          "description": "A unique identifier representing your end-user, which can help system to monitor and detect abuse."
        }
      }
    },
    "llmChatResponse": {
      "type": "object",
      "properties": {
        "id": {
          "type": "string",
          "description": "unique id for the chat completion."
        },
        "created": {
          "type": "string",
          "format": "int64",
          "description": "the unix timestamp (in seconds) of when the chat completion was created."
        },
        "model": {
          "type": "string",
          "title": "the model used for the completion"
        },
        "choices": {
          "type": "array",
          "items": {
            "type": "object",
            "$ref": "#/definitions/llmChatChoice"
          },
          "title": "list of generated completion choices for the input prompt"
        },
        "usage": {
          "$ref": "#/definitions/llmUsage",
          "description": "usage statistics for the completion request."
        }
      }
    },
    "llmChoice": {
      "type": "object",
      "properties": {
        "text": {
          "type": "string",
          "title": "the generated completion"
        },
        "logprobs": {
          "type": "number",
          "format": "float",
          "title": "the log probability of the completion"
        },
        "index": {
          "type": "integer",
          "format": "int64",
          "title": "the index of the generated completion"
        },
        "finishReason": {
          "type": "string",
          "description": "the finish reason of the completion."
        }
      }
    },
    "llmCompletionRequest": {
      "type": "object",
      "properties": {
        "model": {
          "type": "string",
          "title": "ID of the model to use. (required)"
        },
        "prompt": {
          "type": "string",
          "title": "the prompt to generate completions for. (required)"
        },
        "maxTokens": {
          "type": "integer",
          "format": "int64",
          "description": "number of tokens to generate\nthe prompt token count + max_tokens can't exceed the model's max context length."
        },
        "temperature": {
          "type": "number",
          "format": "float",
          "description": "temperature of the sampling, between [0, 2]. default = 1.0\nhigher value will make the ouput more random."
        },
        "topP": {
          "type": "number",
          "format": "float",
          "title": "top_p sampling cutoff, between [0, 1.0]. default = 1.0"
        },
        "n": {
          "type": "integer",
          "format": "int64",
          "title": "number of completions to return for each prompt. default = 1"
        },
        "stream": {
          "type": "boolean",
          "title": "whether to stream partial completions back as they are generated"
        },
        "logprobs": {
          "type": "integer",
          "format": "int64",
          "description": "include the log probabilities of the chosen tokens. the maximum value is 5."
        },
        "echo": {
          "type": "boolean",
          "title": "whether to include the original prompt in the completion response"
        },
        "presencePenalty": {
          "type": "number",
          "format": "float",
          "title": "presence penalty to reduce the likelihood of generating words already in the prompt.\nvalues between [-2.0, 2.0]. Positive values penalize new tokens based on their existing\nin the prompt. default = 0.0"
        },
        "frequencyPenalty": {
          "type": "number",
          "format": "float",
          "description": "frequency penalty to reduce the likelihood of generating the same word multiple times.\nvalues between [0.0, 2.0]. 0.0 means no penalty. default = 0.0\nPositive values penalize new tokens based on their existing frequency in the text."
        },
        "bestOf": {
          "type": "integer",
          "format": "int64",
          "title": "Generates best_of completions server-side and returns the \"best\" (the one with the lowest log probability per token).\nResults can't be streamed once set.\nwhen used with n, best_of controls the number of candidate completions and n specifies how many to return\nbest_of must be \u003e= n"
        },
        "user": {
          "type": "string",
          "description": "A unique identifier representing your end-user, which can help system to monitor and detect abuse."
        }
      }
    },
    "llmCompletionResponse": {
      "type": "object",
      "properties": {
        "id": {
          "type": "string",
          "title": "unique id for the completion request"
        },
        "object": {
          "type": "string",
          "description": "the object type, which is always \"text_completion\"."
        },
        "created": {
          "type": "string",
          "format": "int64",
          "description": "the unix timestamp (in seconds) of when the completion was created."
        },
        "model": {
          "type": "string",
          "title": "the model used for the completion"
        },
        "choices": {
          "type": "array",
          "items": {
            "type": "object",
            "$ref": "#/definitions/llmChoice"
          },
          "title": "list of generated completion choices for the input prompt"
        },
        "usage": {
          "$ref": "#/definitions/llmUsage",
          "description": "usage statistics for the completion request."
        }
      }
    },
    "llmUsage": {
      "type": "object",
      "properties": {
        "promptTokens": {
          "type": "integer",
          "format": "int32",
          "description": "the number of tokens in the prompt."
        },
        "completionTokens": {
          "type": "integer",
          "format": "int32",
          "description": "the number of tokens in the generated completion."
        },
        "totalTokens": {
          "type": "integer",
          "format": "int32",
          "description": "the total number of tokens used in the request (prompt + completion)."
        }
      }
    },
    "protobufAny": {
      "type": "object",
      "properties": {
        "@type": {
          "type": "string"
        }
      },
      "additionalProperties": {}
    },
    "rpcStatus": {
      "type": "object",
      "properties": {
        "code": {
          "type": "integer",
          "format": "int32"
        },
        "message": {
          "type": "string"
        },
        "details": {
          "type": "array",
          "items": {
            "type": "object",
            "$ref": "#/definitions/protobufAny"
          }
        }
      }
    }
  }
}
